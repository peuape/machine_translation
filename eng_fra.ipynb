{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPH1awIEESjp9ywiM3sriG+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peuape/machine_translation/blob/main/eng_fra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATvNvgM3b8kr",
        "outputId": "17dddd62-4849-48d6-c3a2-8a9054683e14"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bleu\n",
            "  Downloading bleu-0.3.tar.gz (5.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting efficiency (from bleu)\n",
            "  Downloading efficiency-2.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from efficiency->bleu) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from efficiency->bleu) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->efficiency->bleu) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->efficiency->bleu) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->efficiency->bleu) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->efficiency->bleu) (1.17.0)\n",
            "Downloading efficiency-2.0-py3-none-any.whl (32 kB)\n",
            "Building wheels for collected packages: bleu\n",
            "  Building wheel for bleu (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bleu: filename=bleu-0.3-py3-none-any.whl size=5780 sha256=eefd0240a9298dfc6ba4067613c14dc34dc538b5b59d482dbe6142de8e1b5994\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/9f/09/3d45ccd4ce42bc796c1f0b960037e30f40b953458d3868b6f3\n",
            "Successfully built bleu\n",
            "Installing collected packages: efficiency, bleu\n",
            "Successfully installed bleu-0.3 efficiency-2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler"
      ],
      "metadata": {
        "id": "rB2RKoXUUxwW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "LqLfN64BoA3t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "j0zC0y3YlSe7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkVTa8iZlUJ3",
        "outputId": "16ca528c-e751-467b-cdd8-eff0bae10f1f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1. Create a class Lang for preprocessing\n",
        "attributes:\n",
        "    str name: language name (\"eng\" or \"fra\")\n",
        "    dict word2index:\n",
        "    dict word2count:\n",
        "    dict index2word: Already has \"SOS\" and \"EOS\" as tokens\n",
        "    int n_words\n",
        "\n",
        "methods:\n",
        "    addWord(word):\n",
        "        updates the attributes of Lang.\n",
        "\n",
        "\n",
        "    addSentence(sentence):\n",
        "        params: str sentence: input/output sentence. Assume theyve already been normalised\n",
        "        Registers new words in the given sentence into Lang with addWord.\n",
        "\"\"\"\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0:\"SOS\", 1:\"EOS\"}\n",
        "        self.n_words = 2\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(\" \"):\n",
        "            self.addWord(word)"
      ],
      "metadata": {
        "id": "ig0AQLBzlY1b"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "2. Create a function to normalise training data by\n",
        " -1 turning them into ascii to remove accents.\n",
        " -2 normalising them\n",
        "\n",
        "unicodeToAscii:\n",
        "    params: str s: unicode string\n",
        "    return: str : ascii string\n",
        "\n",
        "normalizeString:\n",
        "    params: str s: unicode string\n",
        "    return normalised string\n",
        "    Converts unicode into ascii, lowercases, trims, converts .!? into \" \\1\" and removes non_letter characters.\n",
        "    Note that as a result of replacing special characters with a space, there might be sentences ending with a whitespace, which is undesirable.\n",
        "\"\"\"\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r'([.!?])', r' \\1', s)\n",
        "    s = re.sub(r'[^a-zA-Z]', r\" \", s)\n",
        "    return s.strip()\n"
      ],
      "metadata": {
        "id": "HDY1OlFRliBA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "3. Create a function to read data into Lang(create vocabularies)\n",
        "\n",
        "readLang:\n",
        "    params:\n",
        "        str lang1, lang2: language names\n",
        "        bool reverse = False\n",
        "    return:\n",
        "        Lang imput_lang, output_lang\n",
        "        list[list[string]] pairs\n",
        "\n",
        "    -1 Read the file line by line.\n",
        "    -2 Create pairs\n",
        "    -3 Store them in Lang\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def readLang(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    with open(f\"{lang1}-{lang2}.txt\") as f:\n",
        "        lines = f.read().strip().split(\"\\n\")\n",
        "        pairs = [[normalizeString(s) for s in line.split(\"\\t\")] for line in lines]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "    return input_lang, output_lang, pairs\n"
      ],
      "metadata": {
        "id": "KIcrDt4wljRZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 10\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[0].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "metadata": {
        "id": "QKhojRl_lkbG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "4 Preprocess data.\n",
        "    -1 Read data into Lang using readLangs\n",
        "    -2 Filter out long sentences with filterPairs\n",
        "    -3 Fill the vocabulary in the Lang class\n",
        "\n",
        "prepareData:\n",
        "    params:\n",
        "        str lang1, lang2\n",
        "        bool reverse=False\n",
        "    return\n",
        "        Lang input_lang, output_lang\n",
        "        list[list[string]] pairs\n",
        "\"\"\"\n",
        "\n",
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    print(\"Reading data...\")\n",
        "    input_lang, output_lang, pairs = readLang(\"eng\", \"fra\")\n",
        "    print(f\"Read {len(pairs)} sentences. Filtering pairs...\")\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(f\"Trimmed to {len(pairs)} pairs. Creating vocabularies...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "\n",
        "    print(\"Created vocabularies.\")\n",
        "    print(f\"{lang1}: {input_lang.n_words} words\")\n",
        "    print(f\"{lang2}: {output_lang.n_words} words\")\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData(\"eng\", \"fra\")\n",
        "pairs[:10], input_lang.word2index[\"go\"], output_lang.word2index[\"va\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgveIfH5llq9",
        "outputId": "db8f93c3-4675-4fdd-935b-792c7c671c32"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data...\n",
            "Reading lines...\n",
            "Read 135842 sentences. Filtering pairs...\n",
            "Trimmed to 11358 pairs. Creating vocabularies...\n",
            "Created vocabularies.\n",
            "eng: 2981 words\n",
            "fra: 4576 words\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([['i m ok', 'je vais bien'],\n",
              "  ['i m ok', 'ca va'],\n",
              "  ['i m fat', 'je suis gras'],\n",
              "  ['i m fat', 'je suis gros'],\n",
              "  ['i m fit', 'je suis en forme'],\n",
              "  ['i m hit', 'je suis touche'],\n",
              "  ['i m hit', 'je suis touchee'],\n",
              "  ['i m ill', 'je suis malade'],\n",
              "  ['i m sad', 'je suis triste'],\n",
              "  ['i m shy', 'je suis timide']],\n",
              " 648,\n",
              " 6)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "5 Create pytorch classes\n",
        "This nml model is a seq2seq model utilising GRU and attention.\n",
        "The rough architecture is as follows.\n",
        "\n",
        "EncoderRNN:\n",
        "params:\n",
        "    int input_size:dict size\n",
        "    int hidden_size: size of each embedding vector\n",
        "    int dropout_p = 0.1\n",
        "\n",
        "attributes:\n",
        "    hidden_size, embedding, gru, dropout\n",
        "\n",
        "method:\n",
        "    forward:\n",
        "        params;\n",
        "\n",
        "\"\"\"\n",
        "#bahdanau encoder\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(output)\n",
        "        return output, hidden\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9Uyh72Zul0sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "    def forward(self, query, keys):\n",
        "        score = self.Va(torch.tanh(self.Wa(query)+self.Ua(keys)))\n",
        "        score = score.squeeze(2).unsqueeze(1)\n",
        "        weights = F.softmax(score, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "        return context, weights\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(p= dropout_p)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = nn.GRU(2*hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, encoder_output, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_output.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        output_list = []\n",
        "        attention_list = []\n",
        "        for i in  range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(decoder_input, decoder_hidden,encoder_output)\n",
        "            output_list.append(decoder_output)\n",
        "            attention_list.append(attn_weights)\n",
        "\n",
        "            if target_tensor==None:\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()\n",
        "            else:\n",
        "                decoder_input = target_tensor[:,i].unsqueeze(1)\n",
        "\n",
        "        decoder_output = torch.cat(output_list, dim=1)\n",
        "        decoder_output = F.log_softmax(decoder_output, dim=-1)\n",
        "        attention = torch.cat(attention_list, dim=1)\n",
        "        return decoder_output, decoder_hidden, attention\n",
        "\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_output):\n",
        "        query = hidden.permute(1,0,2)\n",
        "        context, weights = self.attention(query, encoder_output)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "        decoder_output, decoder_hidden = self.gru(input_gru, hidden)\n",
        "        decoder_output = self.out(decoder_output)\n",
        "        return decoder_output, decoder_hidden, weights\n"
      ],
      "metadata": {
        "id": "smKNagEUl19T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#luong attention\n",
        "class EncoderLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        hidden, (last_hidden, last_cell) = self.lstm(embedded)\n",
        "        return hidden, (last_hidden, last_cell)\n",
        "\n",
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(DecoderLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.lstmcell = nn.LSTMCell(hidden_size, hidden_size)\n",
        "        self.Vd = nn.Linear(3*hidden_size, hidden_size)\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "        self.He = nn.Linear(2*hidden_size, hidden_size)\n",
        "        self.Ce = nn.Linear(2*hidden_size, hidden_size)\n",
        "        self.Ha = nn.Linear(2*hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, encoder_cell, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        encoder_outputs_projected = self.Ha(encoder_outputs)\n",
        "        encoder_outputs_permuted = torch.permute(encoder_outputs, (0,2,1))\n",
        "        encoder_hidden = torch.cat((encoder_hidden[0], encoder_hidden[1]), dim=-1)\n",
        "        encoder_cell = torch.cat((encoder_cell[0], encoder_cell[1]), dim=-1)\n",
        "        decoder_hidden = self.He(encoder_hidden)\n",
        "        decoder_cell = self.Ce(encoder_cell)\n",
        "        output_list = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden, decoder_cell = self.step(decoder_input, decoder_hidden, decoder_cell, encoder_outputs_permuted, encoder_outputs_projected)\n",
        "            output_list.append(decoder_output.unsqueeze(1))\n",
        "            if target_tensor == None:\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()\n",
        "\n",
        "            else:\n",
        "                decoder_input = target_tensor[:,i]\n",
        "\n",
        "\n",
        "        decoder_output = torch.cat(output_list, dim=1)\n",
        "        decoder_output = F.log_softmax(decoder_output, dim=-1)\n",
        "\n",
        "        return decoder_output, decoder_hidden, decoder_cell\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def step(self, input, decoder_hidden, decoder_cell, encoder_outputs_permuted, encoder_outputs_projected):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        (decoder_hidden, decoder_cell) = self.lstmcell(embedded, (decoder_hidden, decoder_cell))\n",
        "        attention_scores = torch.bmm(encoder_outputs_projected, decoder_hidden.unsqueeze(-1))\n",
        "        attention_scores = F.softmax(attention_scores, dim=1)\n",
        "        attention = torch.bmm(encoder_outputs_permuted, attention_scores)\n",
        "        decoder_output = torch.cat((torch.squeeze(attention, dim=-1), decoder_hidden), dim=1)\n",
        "        decoder_output = self.Vd(decoder_output)\n",
        "        decoder_output = self.dropout(F.tanh(decoder_output))\n",
        "        decoder_output = self.output(decoder_output)\n",
        "        return decoder_output, decoder_hidden, decoder_cell\n",
        "\n"
      ],
      "metadata": {
        "id": "UjoxjBz0PC0w"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(\" \")]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
        "\n",
        "def tensorFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "def get_dataloader(batch_size):\n",
        "    input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "\n",
        "    n = len(pairs)\n",
        "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        inp_ids = indexesFromSentence(input_lang, inp)\n",
        "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                               torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "    return input_lang, output_lang, train_dataloader"
      ],
      "metadata": {
        "id": "XR3iTjkvl28w"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
        "                 decoder_optimizer, criterion,attention=\"luong\"):\n",
        "    total_loss = 0\n",
        "    for data in tqdm(dataloader):\n",
        "        input_tensor, target_tensor = data\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "        if attention==\"bahdanau\":\n",
        "            encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "            decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "        elif attention==\"luong\":\n",
        "            encoder_outputs, (encoder_hidden, encoder_cell) = encoder(input_tensor)\n",
        "            decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, encoder_cell, target_tensor)\n",
        "\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target_tensor.view(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n"
      ],
      "metadata": {
        "id": "jhB4q1ORl4I_"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n"
      ],
      "metadata": {
        "id": "sUKPAmY1l5FS"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "metadata": {
        "id": "bRpLW58Tl6H4"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=1e-3,\n",
        "          print_every=100, plot_every=100):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "metadata": {
        "id": "WGm_vntNl7Tw"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "    return decoded_words, decoder_attn"
      ],
      "metadata": {
        "id": "O-ZdbaPfl8h8"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 128\n",
        "batch_size=32\n",
        "\n",
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErMS_QGcl9oc",
        "outputId": "a6d75a4a-baef-4f00-ac7c-ef3e20312214"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data...\n",
            "Reading lines...\n",
            "Read 135842 sentences. Filtering pairs...\n",
            "Trimmed to 11358 pairs. Creating vocabularies...\n",
            "Created vocabularies.\n",
            "eng: 2981 words\n",
            "fra: 4576 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = EncoderLSTM(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = DecoderLSTM(output_lang.n_words, hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFwVNekzceoH",
        "outputId": "4e43c43d-7575-4ceb-a300-508310db8715"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 355/355 [00:06<00:00, 58.41it/s]\n",
            "100%|██████████| 355/355 [00:07<00:00, 48.17it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 57.03it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 53.65it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 56.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0m 32s (- 8m 8s) (5 6%) 1.9903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 355/355 [00:06<00:00, 54.96it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 54.34it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 56.81it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 52.13it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 58.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1m 4s (- 7m 33s) (10 12%) 1.0614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 355/355 [00:06<00:00, 53.11it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 58.75it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 52.76it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 58.75it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 52.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1m 36s (- 7m 0s) (15 18%) 0.6494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 355/355 [00:06<00:00, 58.60it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 52.75it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 58.12it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 52.92it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 58.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2m 8s (- 6m 25s) (20 25%) 0.4258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 355/355 [00:06<00:00, 53.19it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 58.79it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 52.60it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 57.91it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 53.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2m 40s (- 5m 53s) (25 31%) 0.3071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 355/355 [00:06<00:00, 56.10it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 54.36it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 52.53it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 57.59it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 52.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3m 13s (- 5m 22s) (30 37%) 0.2443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 355/355 [00:06<00:00, 57.80it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 52.08it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 58.54it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 52.43it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 58.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3m 45s (- 4m 49s) (35 43%) 0.2058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 355/355 [00:06<00:00, 52.96it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 58.50it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 52.67it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 58.88it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 53.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4m 17s (- 4m 17s) (40 50%) 0.1845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 355/355 [00:06<00:00, 58.75it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 52.35it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 57.23it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 52.99it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 56.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4m 49s (- 3m 45s) (45 56%) 0.1724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 355/355 [00:06<00:00, 54.80it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 52.99it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 57.61it/s]\n",
            "100%|██████████| 355/355 [00:07<00:00, 49.22it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 55.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5m 22s (- 3m 13s) (50 62%) 0.1627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 355/355 [00:07<00:00, 50.29it/s]\n",
            "100%|██████████| 355/355 [00:07<00:00, 49.96it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 52.25it/s]\n",
            "100%|██████████| 355/355 [00:07<00:00, 50.09it/s]\n",
            "100%|██████████| 355/355 [00:07<00:00, 50.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5m 57s (- 2m 42s) (55 68%) 0.1566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 355/355 [00:07<00:00, 48.96it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 53.66it/s]\n",
            "100%|██████████| 355/355 [00:07<00:00, 49.88it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 56.88it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 51.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6m 31s (- 2m 10s) (60 75%) 0.1506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 355/355 [00:06<00:00, 57.97it/s]\n",
            "100%|██████████| 355/355 [00:07<00:00, 49.99it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 57.85it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 51.77it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 55.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7m 4s (- 1m 37s) (65 81%) 0.1480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 355/355 [00:06<00:00, 54.28it/s]\n",
            "100%|██████████| 355/355 [00:07<00:00, 50.43it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 52.46it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 53.39it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 58.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7m 37s (- 1m 5s) (70 87%) 0.1441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 355/355 [00:06<00:00, 53.08it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 58.70it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 52.79it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 58.46it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 52.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8m 9s (- 0m 32s) (75 93%) 0.1419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 355/355 [00:06<00:00, 56.51it/s]\n",
            "100%|██████████| 355/355 [00:07<00:00, 49.30it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 50.73it/s]\n",
            "100%|██████████| 355/355 [00:06<00:00, 50.95it/s]\n",
            "100%|██████████| 355/355 [00:07<00:00, 50.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8m 44s (- 0m 0s) (80 100%) 0.1407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang, attention=\"luong\"):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        if attention==\"luong\":\n",
        "            encoder_outputs, (encoder_hidden, encoder_cell) = encoder(input_tensor)\n",
        "            decoder_outputs, decoder_hidden, decoder_cell = decoder(encoder_outputs, encoder_hidden, encoder_cell)\n",
        "        else:\n",
        "            encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "            decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "    return decoded_words"
      ],
      "metadata": {
        "id": "9wmtXL_Nl-5D"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n"
      ],
      "metadata": {
        "id": "GEfovLNUrCQj"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bahdanau\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "evaluateRandomly(encoder, decoder)"
      ],
      "metadata": {
        "id": "fXOSfAYBqlbW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6faaae45-c6b6-4f3c-d8ba-98a39a3ca4fb"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> we re punctual\n",
            "= nous sommes ponctuels\n",
            "< nous sommes confrontes a un probleme <EOS>\n",
            "\n",
            "> we re going to search the whole ship\n",
            "= nous allons fouiller la totalite du bateau\n",
            "< nous allons fouiller la totalite du bateau <EOS>\n",
            "\n",
            "> we re almost ready\n",
            "= nous sommes presque pretes\n",
            "< nous sommes presque prets  un film <EOS>\n",
            "\n",
            "> he is amusing himself by playing video games\n",
            "= il s amuse en jouant aux jeux videos\n",
            "< il s amuse en jouant aux jeux videos <EOS>\n",
            "\n",
            "> we re all very good players\n",
            "= nous sommes tous de tres bons joueurs\n",
            "< nous sommes toutes de tres bonnes joueuses <EOS>\n",
            "\n",
            "> you re preaching to the choir\n",
            "= vous prechez des convaincues\n",
            "< tu preches une convaincue de l ecole <EOS>\n",
            "\n",
            "> they re not coming\n",
            "= elles ne viennent pas\n",
            "< ils ne vont pas nous chercher <EOS>\n",
            "\n",
            "> you re too slow\n",
            "= vous etes trop lentes\n",
            "< tu es trop lente pour moi <EOS>\n",
            "\n",
            "> he s still alive\n",
            "= il est toujours en vie\n",
            "< il est encore au lit <EOS>\n",
            "\n",
            "> we re the last\n",
            "= nous sommes les derniers\n",
            "< nous sommes les derniers le dernier espoir <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#bahdanau\n",
        "prediction_list = [evaluate(encoder,decoder,sentence, input_lang, output_lang)[:-1] for sentence in [pair[0] for pair in pairs]]\n",
        "target_list = [pair[1].split(\" \") for pair in pairs]"
      ],
      "metadata": {
        "id": "cSj9lNnNnNVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bahdanau\n",
        "prediction_list = [\" \".join(prediction_list[i]) for i in range(len(prediction_list))]\n",
        "target_list = [\" \".join(target_list[i]) for i in range(len(target_list))]"
      ],
      "metadata": {
        "id": "HY1omk2LaIAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bleu import list_bleu"
      ],
      "metadata": {
        "id": "aZKVYrI4aYnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Bleu score of Bandanau encoder-decoder:\", list_bleu([target_list], prediction_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKPXzUZjbqzk",
        "outputId": "4b318b65-c7a3-4d37-af0d-f033b4e0a49c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] Starting to run this command now: perl /tmp/tmp_bleu/multi-bleu-detok.perl /tmp/tmp_bleu/ref_dtk0.txt < /tmp/tmp_bleu/hyp_dtk0.txt \n",
            "Bleu score of Bandanau encoder-decoder: 73.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#luong\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "evaluateRandomly(encoder, decoder)"
      ],
      "metadata": {
        "id": "35KYU9yO1pcG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2a480bc-3cbc-4b79-c408-f9f3b96122a5"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> i m not in good shape now\n",
            "= je ne suis pas en forme maintenant\n",
            "< je ne suis pas en forme maintenant <EOS>\n",
            "\n",
            "> she s not my type\n",
            "= elle n est pas mon genre\n",
            "< elle n est pas du tout genre a mon genre\n",
            "\n",
            "> i am moving next month\n",
            "= je demenage le mois prochain\n",
            "< je demenage le mois prochain <EOS>\n",
            "\n",
            "> i m so sorry that i lied to you\n",
            "= je suis si desole de t avoir menti\n",
            "< je suis si desole de t avoir menti <EOS>\n",
            "\n",
            "> you are as white as a sheet\n",
            "= vous etes blanc comme un drap\n",
            "< tu es blanc comme un cachet d aspirine <EOS>\n",
            "\n",
            "> you re extroverted\n",
            "= vous etes extravertie\n",
            "< tu es un critique de bureau <EOS>\n",
            "\n",
            "> you re out of booze\n",
            "= c est maree basse\n",
            "< tu es hors de son affaire <EOS>\n",
            "\n",
            "> i m very tired\n",
            "= je suis fourbu\n",
            "< je suis tres fatigue par tres fatigue <EOS>\n",
            "\n",
            "> i m not easily impressed\n",
            "= je ne suis pas facilement impressionne\n",
            "< je ne suis pas facilement impressionne a ce sujet <EOS>\n",
            "\n",
            "> we re not friends anymore\n",
            "= nous ne sommes plus amies\n",
            "< nous ne sommes plus amies de tous <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#luong attention\n",
        "prediction_list = [evaluate(encoder,decoder,sentence, input_lang, output_lang)[:-1] for sentence in [pair[0] for pair in pairs]]\n",
        "target_list = [pair[1].split(\" \") for pair in pairs]\n",
        "prediction_list = [\" \".join(prediction_list[i]) for i in range(len(prediction_list))]\n",
        "target_list = [\" \".join(target_list[i]) for i in range(len(target_list))]"
      ],
      "metadata": {
        "id": "NR7OnWcoiOZB"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Bleu score of Luong encoder-decoder:\", list_bleu([target_list], prediction_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZVmYn3CiXub",
        "outputId": "e82aac92-1b07-44e3-c5f4-4fdfb1322dea"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] Starting to run this command now: perl /tmp/tmp_bleu/multi-bleu-detok.perl /tmp/tmp_bleu/ref_dtk0.txt < /tmp/tmp_bleu/hyp_dtk0.txt \n",
            "Bleu score of Luong encoder-decoder: 40.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#luong\n",
        "prediction_list[:10], target_list[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8mv8UPJifNu",
        "outputId": "6088966b-8a3e-434f-a24b-012018b00532"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['je crains d avoir a tom',\n",
              "  'je crains d avoir a tom',\n",
              "  'je suis content que tu aies aime la tete',\n",
              "  'je suis content que tu aies aime la tete',\n",
              "  'je suis en route pour forme',\n",
              "  'je suis petite par l anglais pour moi',\n",
              "  'je suis petite par l anglais pour moi',\n",
              "  'je suis sure d etre un bon malade',\n",
              "  'je suis triste d etre triste a aies',\n",
              "  'je suis timide d etre difficile'],\n",
              " ['je vais bien',\n",
              "  'ca va',\n",
              "  'je suis gras',\n",
              "  'je suis gros',\n",
              "  'je suis en forme',\n",
              "  'je suis touche',\n",
              "  'je suis touchee',\n",
              "  'je suis malade',\n",
              "  'je suis triste',\n",
              "  'je suis timide'])"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    }
  ]
}